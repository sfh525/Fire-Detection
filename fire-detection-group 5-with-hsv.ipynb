{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10025638,"sourceType":"datasetVersion","datasetId":6173942}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y opencv-python opencv-python-headless\n!pip install opencv-contrib-python-headless==4.5.5.64","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom skimage.feature import blob_dog, local_binary_pattern\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nimport xgboost as xgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 1: Acquire Video Paths and Labels ---\ndef acquire_videos_and_labels(dataset_path):\n    video_paths = []\n    labels = []\n    class_labels = {\n        \"AccidentsInaTunnel\": 0,\n        \"FireClips\": 1,\n        \"OtherClips\": 2,\n        \"Smoke_Far\": 3,\n        \"SmokeClips\": 4\n    }\n    for folder_name, label in class_labels.items():\n        folder_path = os.path.join(dataset_path, folder_name)\n        for video_file in os.listdir(folder_path):\n            if video_file.endswith(\".mp4\") or video_file.endswith(\".avi\"):\n                video_paths.append(os.path.join(folder_path, video_file))\n                labels.append(label)\n    return video_paths, labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 2: Acquire Frames from a Single Video ---\ndef acquire_frames_from_video(video_path):\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n    cap.release()\n    return frames","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 3: Enhancement ---\ndef enhance_image(image):\n    gaussian_filtered = cv2.GaussianBlur(image, (5, 5), 0)\n    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n    sharpened_image = cv2.filter2D(gaussian_filtered, -1, kernel)\n    return sharpened_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 4: Segmentation ---\ndef segment_image(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    _, thresholded = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return thresholded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 5: Convex Hull ---\ndef apply_convex_hull(segmented_image):\n    contours, _ = cv2.findContours(segmented_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    hulls = [cv2.convexHull(contour) for contour in contours]\n    hull_image = np.zeros_like(segmented_image)\n    cv2.drawContours(hull_image, hulls, -1, (255), thickness=cv2.FILLED)\n    return hull_image, hulls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 6: Detect Blobs ---\ndef detect_blobs(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blobs = blob_dog(gray, min_sigma=1, max_sigma=50, threshold=0.2)\n    # Flatten blob coordinates into a single feature vector\n    blob_features = np.array([len(blobs)])  # Number of blobs detected\n    return blob_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 7: Feature Extraction ---\ndef extract_lbp_histogram(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(257), range=(0, 256))\n    hist = hist.astype(\"float\")\n    hist /= hist.sum()  # Normalize\n    return hist","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_gabor_filter(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    kernels = [cv2.getGaborKernel((21, 21), 5.0, theta * np.pi / 4.0, 10.0, 0.5, 0) for theta in range(4)]\n    filtered_images = [cv2.filter2D(gray, cv2.CV_32F, k) for k in kernels]\n    combined_gabor = np.max(filtered_images, axis=0)\n    return combined_gabor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_optical_flow(prev_frame, next_frame):\n    # Resize next_frame if dimensions don't match\n    if prev_frame.shape[:2] != next_frame.shape[:2]:\n        next_frame = cv2.resize(next_frame, (prev_frame.shape[1], prev_frame.shape[0]))\n    \n    # Convert frames to grayscale\n    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n    next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n\n    # Compute optical flow\n    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n\n    # Calculate magnitude and angle of the flow\n    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n\n    # Create a normalized histogram of the magnitude\n    flow_hist, _ = np.histogram(magnitude.ravel(), bins=64, range=(0, 255))\n    flow_hist = flow_hist.astype(\"float\") / (flow_hist.sum() + 1e-8)  # Prevent division by zero\n\n    return flow_hist","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_hsv_features_and_moments(image):\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    # Compute histograms for H, S, and V channels\n    h_hist = cv2.calcHist([hsv], [0], None, [64], [0, 180])  # Hue range: 0-180\n    s_hist = cv2.calcHist([hsv], [1], None, [64], [0, 256])  # Saturation range: 0-256\n    v_hist = cv2.calcHist([hsv], [2], None, [64], [0, 256])  # Value range: 0-256\n\n    # Normalize histograms\n    h_hist = h_hist / (h_hist.sum() + 1e-8)\n    s_hist = s_hist / (s_hist.sum() + 1e-8)\n    v_hist = v_hist / (v_hist.sum() + 1e-8)\n\n    # Compute mean, variance, and skewness for each channel\n    h_mean = h_hist.mean()\n    h_var = h_hist.var()\n    h_skew = ((h_hist - h_mean) ** 3).mean() ** (1 / 3)\n\n    s_mean = s_hist.mean()\n    s_var = s_hist.var()\n    s_skew = ((s_hist - s_mean) ** 3).mean() ** (1 / 3)\n\n    v_mean = v_hist.mean()\n    v_var = v_hist.var()\n    v_skew = ((v_hist - v_mean) ** 3).mean() ** (1 / 3)\n\n    # Flatten and concatenate histograms and moments\n    hsv_hist_features = np.concatenate([h_hist.flatten(), s_hist.flatten(), v_hist.flatten()])\n    color_moments = np.array([h_mean, h_var, h_skew, s_mean, s_var, s_skew, v_mean, v_var, v_skew])\n\n    # Combine histograms and moments\n    hsv_features = np.concatenate([hsv_hist_features, color_moments])\n    return hsv_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:36:35.071672Z","iopub.execute_input":"2024-11-28T13:36:35.072020Z","iopub.status.idle":"2024-11-28T13:36:35.080330Z","shell.execute_reply.started":"2024-11-28T13:36:35.071987Z","shell.execute_reply":"2024-11-28T13:36:35.079320Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --- Step 9: Update Represent Features ---\ndef represent_features(frame, next_frame):\n    enhanced_frame = enhance_image(frame)\n    segmented_frame = segment_image(enhanced_frame)\n    hull_image, hulls = apply_convex_hull(segmented_frame)\n    lbp_hist = extract_lbp_histogram(enhanced_frame)\n    gabor_filtered = apply_gabor_filter(enhanced_frame)\n    gabor_hist, _ = np.histogram(gabor_filtered.ravel(), bins=64, range=(0, 255))\n    gabor_hist = gabor_hist.astype(\"float\") / gabor_hist.sum()\n    flow_hist = compute_optical_flow(frame, next_frame)\n    blob_features = detect_blobs(enhanced_frame)\n    hsv_features = extract_hsv_features_and_moments(frame)  # Add HSV features with color moments\n\n    # Combine all features\n    combined_features = np.hstack([lbp_hist, gabor_hist, flow_hist, blob_features, hsv_features])\n    return combined_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T13:36:37.626306Z","iopub.execute_input":"2024-11-28T13:36:37.626593Z","iopub.status.idle":"2024-11-28T13:36:37.632651Z","shell.execute_reply.started":"2024-11-28T13:36:37.626568Z","shell.execute_reply":"2024-11-28T13:36:37.631535Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def process_videos(video_paths, labels):\n    all_features = []\n    all_labels = []\n    for video_path, label in zip(video_paths, labels):\n        frames = acquire_frames_from_video(video_path)\n        for i in range(len(frames) - 1):  # Process pairs of frames for optical flow\n            feature_vector = represent_features(frames[i], frames[i + 1])\n            all_features.append(feature_vector)\n            all_labels.append(label)\n    return np.array(all_features), np.array(all_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate(X_train, y_train, X_test, y_test):\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n\n    params = {\n        \"objective\": \"multi:softmax\",\n        \"num_class\": 5,\n        \"tree_method\": \"gpu_hist\",  # Enable GPU\n        \"predictor\": \"gpu_predictor\",\n        \"max_depth\": 6,\n        \"learning_rate\": 0.1,\n        \"n_estimators\": 100,\n    }\n    \n    model = xgb.train(params, dtrain, num_boost_round=100)\n    y_pred = model.predict(dtest)\n\n    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Main Script ---\ndataset_path = \"/kaggle/input/visifire\"\nvideo_paths, video_labels = acquire_videos_and_labels(dataset_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split dataset at video level\ntrain_videos, test_videos, train_labels, test_labels = train_test_split(\n    video_paths, video_labels, test_size=0.2, random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process training and test sets\nX_train, y_train = process_videos(train_videos, train_labels)\nX_test, y_test = process_videos(test_videos, test_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_evaluate(X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}